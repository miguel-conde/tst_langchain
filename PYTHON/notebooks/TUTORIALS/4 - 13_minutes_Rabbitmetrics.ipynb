{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Model (LLM wrappers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLarge language models are deep neural networks that are trained to generate contextualized representations of words, phrases, and sentences.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run basic query with OpenAI wrapper\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = 'text-davinci-003')\n",
    "llm('explain large language models in a sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.3)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are an expert data scientist'),\n",
    "    HumanMessage(content='Write a Python script that trains a neural network on simulated data')\n",
    "]\n",
    "\n",
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's an example script that trains a simple neural network on simulated data using the Keras library:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "\n",
      "# Generate some simulated data\n",
      "X = np.random.rand(1000, 10)\n",
      "y = np.sum(X, axis=1)\n",
      "\n",
      "# Define the neural network architecture\n",
      "model = Sequential()\n",
      "model.add(Dense(32, input_dim=10, activation='relu'))\n",
      "model.add(Dense(1, activation='linear'))\n",
      "\n",
      "# Compile the model\n",
      "model.compile(loss='mean_squared_error', optimizer='adam')\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y, epochs=100, batch_size=32)\n",
      "\n",
      "# Evaluate the model on some test data\n",
      "X_test = np.random.rand(100, 10)\n",
      "y_test = np.sum(X_test, axis=1)\n",
      "loss = model.evaluate(X_test, y_test)\n",
      "print('Test loss:', loss)\n",
      "```\n",
      "\n",
      "This script generates a random matrix `X` with 1000 rows and 10 columns, and a corresponding vector `y` that is the sum of each row of `X`. It then defines a neural network with one hidden layer of 32 units and a linear output layer. The model is compiled with the mean squared error loss function and the Adam optimizer. The model is trained for 100 epochs on the simulated data, with a batch size of 32. Finally, the model is evaluated on a separate test set of 100 samples, and the test loss is printed.\n"
     ]
    }
   ],
   "source": [
    "print(response.content, end ='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompt and define PromptTemplat\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models.\n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['concept'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], output_parser=None, partial_variables={}, template='\\nYou are an expert data scientist with an expertise in building deep learning models.\\nExplain the concept of {concept} in a couple of lines\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegularization is a technique used to reduce the complexity of a model by penalizing certain parameters to prevent them from becoming too large. This helps to reduce the overfitting of the model and improve the generalization of the model to unseen data.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run LLM with PromptTemplate\n",
    "\n",
    "llm(prompt.format(concept='regularization'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAutoencoders are a type of neural network that learn to compress and reconstruct input data from its own output. They are used for unsupervised learning tasks, such as dimensionality reduction, denoising, and feature extraction.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run LLM with PromptTemplate\n",
    "\n",
    "llm(prompt.format(concept='autoencoder'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autoencoders are a type of neural network that use an encoding-decoding architecture to learn an efficient representation of input data. This representation is used to reconstruct the original input data, and the network is trained to optimize the reconstruction error. Autoencoders are popular for data compression, feature learning, and anomaly detection.\n"
     ]
    }
   ],
   "source": [
    "# Import LLMChain and define chain with language model and prompt as arguments.\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run('autoencoder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a second prompt \n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=['ml_concept'],\n",
    "    template = \"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. It is composed of an encoder, which learns to map the input data into a hidden representation, and a decoder, which learns to reconstruct the input data from the hidden representation.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "An autoencoder is like an invisible robot that helps us figure out how to store information in the most efficient way. To do this, it uses something called artificial neural networks. \n",
      "\n",
      "Artificial neural networks are like a complex network of computers that can process information in a very smart way. The autoencoder uses this network to learn how to store information in the most efficient way possible. \n",
      "\n",
      "The autoencoder is made up of two parts: an encoder and a decoder. The encoder's job is to take the information and figure out how to store it in the most efficient way. It does this by creating a hidden representation of the data. This hidden representation is like a secret code that the computer can use to store the data in the most efficient way. \n",
      "\n",
      "The decoder's job is to take the hidden representation and figure out how to turn it back into the original data. This is like a secret code that the computer can use to turn the hidden representation back into the original data.\n",
      "\n",
      "In other words, the autoencoder takes information, creates a hidden representation of it, and then takes the hidden representation and turns it back into the original information. This helps us store information in\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "An autoencoder is like an invisible robot that helps us figure out how to store information in the most efficient way. To do this, it uses something called artificial neural networks. \n",
      "\n",
      "Artificial neural networks are like a complex network of computers that can process information in a very smart way. The autoencoder uses this network to learn how to store information in the most efficient way possible. \n",
      "\n",
      "The autoencoder is made up of two parts: an encoder and a decoder. The encoder's job is to take the information and figure out how to store it in the most efficient way. It does this by creating a hidden representation of the data. This hidden representation is like a secret code that the computer can use to store the data in the most efficient way. \n",
      "\n",
      "The decoder's job is to take the hidden representation and figure out how to turn it back into the original data. This is like a secret code that the computer can use to turn the hidden representation back into the original data.\n",
      "\n",
      "In other words, the autoencoder takes information, creates a hidden representation of it, and then takes the hidden representation and turns it back into the original information. This helps us store information in\n"
     ]
    }
   ],
   "source": [
    "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "explanation = overall_chain.run(\"autoencoder\")\n",
    "print(explanation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and VectorStores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([explanation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An autoencoder is like an invisible robot that helps us figure out how to store information in the'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Individual text chunks can be accessed with \"page_content\"\n",
    "\n",
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and instantiate OpenAI embeddings\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"ada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the first text chunk into a vector with the embedding\n",
    "\n",
    "query_result = embeddings.embed_query(texts[0].page_content)\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\migue\\Documents\\PROYECTOS DATA SCIENCE\\langchain\\.venv\\Lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import and initialize Pinecone client\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),  \n",
    "    environment=os.getenv('PINECONE_ENV')  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload vectors to Pinecone\n",
    "\n",
    "index_name = \"langchain-quickstart\"\n",
    "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='efficient way possible.', metadata={}), Document(page_content='An autoencoder is like an invisible robot that helps us figure out how to store information in the', metadata={}), Document(page_content='information and figure out how to store it in the most efficient way. It does this by creating a', metadata={}), Document(page_content='computer can use to store the data in the most efficient way.', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Do a simple vector similarity search\n",
    "\n",
    "query = \"What is magical about an autoencoder?\"\n",
    "result = search.similarity_search(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python REPL tool and instantiate Python agent\n",
    "\n",
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to solve a quadratic equation\n",
      "Action: Python REPL\n",
      "Action Input: import numpy as np\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use numpy to solve the equation\n",
      "Action: Python REPL\n",
      "Action Input: np.roots([3,2,-1])\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: (-1.0, 0.3333333333333333)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(-1.0, 0.3333333333333333)'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the Python agent\n",
    "\n",
    "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
